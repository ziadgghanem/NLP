{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f49867",
   "metadata": {},
   "source": [
    "DeepPavlov Classification Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c1f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "#from tensorflow import keras\n",
    "#import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40da3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.15.0\n",
      "Numpy version 1.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "print(\"Tensorflow version {}\".format(tf.__version__))\n",
    "print(\"Numpy version {}\".format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22fb8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\fghanem.FADIGHANEM\\\\Jupyter'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd #'C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\intent_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b1291f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>listen to westbam alumb allergic on google music</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add step to me to the 50 clásicos playlist</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i give this current textbook a rating value of...</td>\n",
       "      <td>RateBook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>play the song little robin redbreast</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>please add iris dement to my playlist this is ...</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         intent\n",
       "0   listen to westbam alumb allergic on google music      PlayMusic\n",
       "1         add step to me to the 50 clásicos playlist  AddToPlaylist\n",
       "2  i give this current textbook a rating value of...       RateBook\n",
       "3               play the song little robin redbreast      PlayMusic\n",
       "4  please add iris dement to my playlist this is ...  AddToPlaylist"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('intent_r/train_r.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685ca9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classes.dict', 'test_r.csv', 'tokens.dict', 'train_r.csv', 'valid_r.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('intent_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592563e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BasicClassificationDatasetReader to read csv data\n",
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader\n",
    "# read data from particular columns of `.csv` file\n",
    "dr = BasicClassificationDatasetReader().read(\n",
    "    data_path= './intent_r/',\n",
    "    train='train_r.csv',\n",
    "    test = 'test_r.csv',\n",
    "    valid = 'valid_r.csv',\n",
    "    x = 'text',\n",
    "    y = 'intent'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212859e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns type: dict\n",
    "type(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a023673c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('train', 13084), ('valid', 700), ('test', 700)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check train/valid/test sizes\n",
    "[(k, len(dr[k])) for k in dr.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8d57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BasicClassificationDatasetIterator\n",
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105b4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = BasicClassificationDatasetIterator(data=dr)\n",
    "# can enter any other data type including `all`\n",
    "x_train, y_train = train_iterator.get_instances(data_type='train')\n",
    "x_test, y_test = train_iterator.get_instances(data_type='test')\n",
    "x_valid, y_valid = train_iterator.get_instances(data_type='valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8794fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: listen to westbam alumb allergic on google music\n",
      "y: PlayMusic\n",
      "=================\n",
      "x: add step to me to the 50 clásicos playlist\n",
      "y: AddToPlaylist\n",
      "=================\n",
      "x: i give this current textbook a rating value of 1 and a best rating of 6\n",
      "y: RateBook\n",
      "=================\n",
      "x: play the song little robin redbreast\n",
      "y: PlayMusic\n",
      "=================\n",
      "x: please add iris dement to my playlist this is selena\n",
      "y: AddToPlaylist\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "for x, y in list(zip(x_train, y_train))[:5]:\n",
    "    print('x:', x)\n",
    "    print('y:', y)\n",
    "    print('=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e884b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# use str_lower tp lowercase texts.\n",
    "from deeppavlov.models.preprocessors.str_lower import str_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cefb2d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i wonder if deeppavlov can make this sentence lowercase.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lower(['I wonder if DeepPavlov can make this sentence Lowercase.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c2bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTKTokenizer splits string to tokens.\n",
    "from deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "746ae64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I',\n",
       "  'wonder',\n",
       "  'if',\n",
       "  'DeepPavlov',\n",
       "  'can',\n",
       "  'make',\n",
       "  'this',\n",
       "  'sentence',\n",
       "  'Lowercase',\n",
       "  '.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = NLTKMosesTokenizer()\n",
    "tokenizer(['I wonder if DeepPavlov can make this sentence Lowercase.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5cccda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize train text data\n",
    "train_x_lower_tokenized = str_lower(tokenizer(train_iterator.get_instances(data_type='train')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8979094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6b67c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:33:23.626 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\intent_r\\classes.dict]\n"
     ]
    }
   ],
   "source": [
    "# initialize simple vocabulary to collect all appeared in the dataset classes\n",
    "classes_vocab = SimpleVocabulary(\n",
    "    save_path='./intent_r/classes.dict',\n",
    "    load_path='./intent_r/classes.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4bcb809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:33:26.816 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\intent_r\\classes.dict]\n"
     ]
    }
   ],
   "source": [
    "classes_vocab.fit((train_iterator.get_instances(data_type='train')[1]))\n",
    "classes_vocab.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e3f41c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PlayMusic', 0),\n",
       " ('GetWeather', 1),\n",
       " ('BookRestaurant', 2),\n",
       " ('RateBook', 3),\n",
       " ('SearchScreeningEvent', 4),\n",
       " ('SearchCreativeWork', 5),\n",
       " ('AddToPlaylist', 6)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class labels\n",
    "list(classes_vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27bdb9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:33:28.474 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\intent_r\\tokens.dict]\n"
     ]
    }
   ],
   "source": [
    "# also one can collect vocabulary of textual tokens appeared 2 and more times in the dataset\n",
    "token_vocab = SimpleVocabulary(\n",
    "    save_path='./intent_r/tokens.dict',\n",
    "    load_path='./intent_r/tokens.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>', '<UNK>',),\n",
    "    unk_token='<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa6b54a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:33:29.984 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\intent_r\\tokens.dict]\n"
     ]
    }
   ],
   "source": [
    "token_vocab.fit(train_x_lower_tokenized)\n",
    "token_vocab.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4f21181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4440"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens in dictionary\n",
    "len(token_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01769abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7054),\n",
       " ('a', 4205),\n",
       " ('in', 3275),\n",
       " ('to', 3209),\n",
       " ('for', 2896),\n",
       " ('of', 2449),\n",
       " ('i', 2132),\n",
       " ('at', 2006),\n",
       " ('play', 1826),\n",
       " ('is', 1551)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 most common words and number of times their appeared\n",
    "token_vocab.freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0067469e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 1, 236, 1, 39, 183, 20, 1, 1, 1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = token_vocab(str_lower(tokenizer(['I wonder if DeepPavlov can make this sentence Lowercase.'])))\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3777a574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i <UNK> if <UNK> can make this <UNK> <UNK> <UNK>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(token_vocab(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30672e2e",
   "metadata": {},
   "source": [
    "###### Featurization (embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78aeb2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BoWEmbedder for bag of words embedding\n",
    "## bow converts text into tuples equal in length to size of vocab\n",
    "### ith pos is 1 if i is in the text 0 else\n",
    "#### does not account for order\n",
    "import numpy as np\n",
    "from deeppavlov.models.embedders.bow_embedder import BoWEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85eaf008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 0, ..., 0, 0, 0])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize bag-of-words embedder giving total number of tokens\n",
    "bow = BoWEmbedder(depth=token_vocab.len)\n",
    "# it assumes indexed tokenized samples\n",
    "bow(token_vocab(str_lower(tokenizer(['Is it freezing in Offerman, California?']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "520ea608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num tokens in in the vocabulary\n",
    "sum(bow(token_vocab(str_lower(tokenizer(['Is it freezing in Offerman, California?']))))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74c6aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SklearnComponent is a universal wrapper for any vecotirzer/estimator \n",
    "from deeppavlov.models.sklearn import SklearnComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb7360ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:33:42.852 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\tfidf_v0.pkl\n",
      "C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.21.2 when using version 0.18.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.21.2 when using version 0.18.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "2021-05-27 16:33:42.870 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2021-05-27 16:33:42.870 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "# initialize TF-IDF vectorizer sklearn component with `transform` as infer method\n",
    "tfidf = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_v0.pkl',\n",
    "    load_path='./tfidf_v0.pkl',\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36533cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:33:48.128 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2021-05-27 16:33:48.310 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\tfidf_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit on textual train instances and save it\n",
    "tfidf.fit(str_lower(train_iterator.get_instances(data_type='train')[0]))\n",
    "tfidf.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b2aea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x11027 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(str_lower(['Is it freezing in Offerman, California?']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22d8868f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11027"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens in the TF-IDF vocabulary\n",
    "len(tfidf.model.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2658ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GloVeEmbedder: unsupervised learning algo for obtaining vector rep for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af747c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version 0.18.2\n",
      "scipy version 1.2.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import scipy\n",
    "\n",
    "# issues with scipy used below fix\n",
    "## pip install scipy==1.2.0\n",
    "## pip install scikit-learn==0.18.2\n",
    "\n",
    "print(\"sklearn version {}\".format(sklearn.__version__))\n",
    "print(\"scipy version {}\".format(scipy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40454f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\Python\\Python36\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.embedders.glove_embedder import GloVeEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e9e3c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65125211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists.\n"
     ]
    }
   ],
   "source": [
    "#GloVe embedding file\n",
    "from deeppavlov.core.data.utils import simple_download\n",
    "if path.exists(\"./glove.6B.100d.txt\"):\n",
    "    print(\"file already exists.\")\n",
    "else: \n",
    "    simple_download(url=\"http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt\", \n",
    "                destination=\"./glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07dc3af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:36:45.150 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 52: [loading GloVe embeddings from `C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\glove.6B.100d.txt`]\n"
     ]
    }
   ],
   "source": [
    "embedder = GloVeEmbedder(load_path='./glove.6B.100d.txt',\n",
    "                         dim=100, pad_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcb4ec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, (100,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape is (batch_size x max_num_tokens_in_the_batch x embedding_dim)\n",
    "embedded_batch = embedder(str_lower(tokenizer(['Is it freezing in Offerman, California?']))) \n",
    "len(embedded_batch), len(embedded_batch[0]), embedded_batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f073941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True False IDF Embedder\n",
    "from deeppavlov.models.embedders.tfidf_weighted_embedder import TfidfWeightedEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e42c117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_embedder = TfidfWeightedEmbedder(\n",
    "    embedder=embedder,  # our GloVe embedder instance\n",
    "    tokenizer=tokenizer,  # our tokenizer instance\n",
    "    mean=True,  # to return one vector per sample\n",
    "    vectorizer=tfidf  # our TF-IDF vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d4950da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, (100,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape is (batch_size x  embedding_dim)\n",
    "embedded_batch = weighted_embedder(str_lower(tokenizer(['Is it freezing in Offerman, California?']))) \n",
    "len(embedded_batch), embedded_batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ddbf1",
   "metadata": {},
   "source": [
    "###### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7c66864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.metrics.accuracy import sets_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10caa172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train and valid data from iterator\n",
    "x_train, y_train = train_iterator.get_instances(data_type=\"train\")\n",
    "x_valid, y_valid = train_iterator.get_instances(data_type=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a042440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:38:55.461 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\logreg_v0.pkl\n",
      "2021-05-27 16:38:55.592 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2021-05-27 16:38:55.592 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "# initialize sklearn classifier, all parameters for classifier could be passed\n",
    "cls = SklearnComponent(\n",
    "    model_class=\"sklearn.linear_model:LogisticRegression\",\n",
    "    infer_method=\"predict\",\n",
    "    save_path='./logreg_v0.pkl',\n",
    "    load_path='./logreg_v0.pkl',\n",
    "    C=1,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6dd965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:38:59.235 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "2021-05-27 16:38:59.543 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\logreg_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(tfidf(x_train), y_train)\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9c80da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddToPlaylist\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred = cls(tfidf(x_valid))\n",
    "print(y_valid_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e26a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:39:04.759 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "2021-05-27 16:39:05.58 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to C:\\Users\\fghanem.FADIGHANEM\\Jupyter\\logreg_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(tfidf(x_train), y_train)\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78562436",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = cls(tfidf(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2d96d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: i d like to have this track onto my classical relaxations playlist\n",
      "True label: AddToPlaylist\n",
      "Predicted label: AddToPlaylist\n"
     ]
    }
   ],
   "source": [
    "# Let's look into obtained result\n",
    "print(\"Text sample: {}\".format(x_valid[0]))\n",
    "print(\"True label: {}\".format(y_valid[0]))\n",
    "print(\"Predicted label: {}\".format(y_valid_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d94030e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate sets accuracy (because each element is a list of labels)\n",
    "sets_accuracy(np.squeeze(y_valid), y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12717c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with Tensorflow version 1.15.0\n",
    "# works with Numpy version 1.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50cde358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.classifiers.keras_classification_model import KerasClassificationModel\n",
    "from deeppavlov.models.preprocessors.one_hotter import OneHotter\n",
    "from deeppavlov.models.classifiers.proba2labels import Proba2Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0d3f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fghanem.FADIGHANEM\\AppData\\Roaming\\Python\\Python36\\site-packages\\deeppavlov\\core\\models\\tf_backend.py:38: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:48:19.952 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 216: [initializing `KerasClassificationModel` from scratch as cnn_model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fghanem.FADIGHANEM\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:48:20.453 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 129: Model was successfully initialized!\n",
      "Model summary:\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 15, 100)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 15, 128)      38528       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 128)      64128       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 128)      89728       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 15, 128)      512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 128)      512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 128)      512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 15, 128)      0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 128)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 128)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 384)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 384)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          38500       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100)          400         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 7)            707         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 7)            28          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 7)            0           batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 233,555\n",
      "Trainable params: 232,573\n",
      "Non-trainable params: 982\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Intialize `KerasClassificationModel` that composes CNN shallow-and-wide network \n",
    "# (name here as`cnn_model`)\n",
    "cls = KerasClassificationModel(save_path=\"./cnn_model_v0\", \n",
    "                               load_path=\"./cnn_model_v0\", \n",
    "                               embedding_size=embedder.dim,\n",
    "                               n_classes=classes_vocab.len,\n",
    "                               model_name=\"cnn_model\",\n",
    "                               text_size=15, # number of tokens\n",
    "                               kernel_sizes_cnn=[3, 5, 7],\n",
    "                               filters_cnn=128,\n",
    "                               dense_size=100,\n",
    "                               optimizer=\"Adam\",\n",
    "                               learning_rate=0.1,\n",
    "                               learning_rate_decay=0.01,\n",
    "                               loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab5a01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `KerasClassificationModel` assumes one-hotted distribution of classes per sample.\n",
    "# `OneHotter` converts indices to one-hot vectors representation.\n",
    "#  To obtain indices we can use our `classes_vocab` intialized and fitted above\n",
    "onehotter = OneHotter(depth=classes_vocab.len, single_vector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97ef0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fghanem.FADIGHANEM\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Train for 10 epochs\n",
    "for ep in range(10):\n",
    "    for x, y in train_iterator.gen_batches(batch_size=64, \n",
    "                                           data_type=\"train\"):\n",
    "        x_embed = embedder(tokenizer(str_lower(x)))\n",
    "        y_onehot = onehotter(classes_vocab(y))\n",
    "        cls.train_on_batch(x_embed, y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "368651d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 16:49:58.736 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 346: [saving model to cnn_model_v0_opt.json]\n"
     ]
    }
   ],
   "source": [
    "# Save model weights and parameters\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7f0ba73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deeppavlov.models.classifiers.keras_classification_model.KerasClassificationModel at 0x19b869d00f0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "94b5789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor Tensor(\"activation_4/Sigmoid:0\", shape=(?, 7), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-6b97772ba57c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_valid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_lower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_valid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#notworking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\deeppavlov\\models\\classifiers\\keras_classification_model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \"\"\"\n\u001b[0;32m    203\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_model_from_scratch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m   \u001b[1;31m# function we recompile the metrics based on the updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m   \u001b[1;31m# sample_weight_mode value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m   \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m   \u001b[1;31m# Prepare validation data. Hold references to the iterator and the input list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[1;34m(model, mode)\u001b[0m\n\u001b[0;32m    564\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m   2187\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2189\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2190\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_predict_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2177\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2178\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'predict_function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2179\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   2180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2181\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[0;32m   3676\u001b[0m                'backend') % key\n\u001b[0;32m   3677\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3678\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mGraphExecutionFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[0;32m   3328\u001b[0m     \u001b[1;31m# dependencies in call.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3329\u001b[0m     \u001b[1;31m# Index 0 = total loss or model output for `predict`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3330\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3331\u001b[0m       \u001b[0mupdates_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3332\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[1;34m(control_inputs)\u001b[0m\n\u001b[0;32m   5252\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mNullContextmanager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5253\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5254\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontrol_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[1;34m(self, control_inputs)\u001b[0m\n\u001b[0;32m   4686\u001b[0m           (hasattr(c, \"_handle\") and hasattr(c, \"op\"))):\n\u001b[0;32m   4687\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4688\u001b[1;33m       \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4689\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4690\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3606\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3607\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3609\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dp\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3684\u001b[0m       \u001b[1;31m# Actually obj is just the object it's referring to.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3685\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3686\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3687\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3688\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOperation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor Tensor(\"activation_4/Sigmoid:0\", shape=(?, 7), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "x_valid_pred = embedder(tokenizer(str_lower(x_valid)))\n",
    "print(type(x_valid_pred))\n",
    "y_valid_pred = cls(x_valid_pred)#notworking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b3cdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2labels = Proba2Labels(max_proba=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a2d6603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: i d like to have this track onto my classical relaxations playlist\n",
      "True label: AddToPlaylist\n",
      "Predicted probability distribution: {'PlayMusic': 'A', 'GetWeather': 'd', 'BookRestaurant': 'd', 'RateBook': 'T', 'SearchScreeningEvent': 'o', 'SearchCreativeWork': 'P', 'AddToPlaylist': 'l'}\n",
      "Predicted label: PlayMusic\n"
     ]
    }
   ],
   "source": [
    "# Let's look into obtained result\n",
    "print(\"Text sample: {}\".format(x_valid[0]))\n",
    "print(\"True label: {}\".format(y_valid[0]))\n",
    "print(\"Predicted probability distribution: {}\".format(dict(zip(classes_vocab.keys(), \n",
    "                                                               y_valid_pred[0]))))\n",
    "print(\"Predicted label: {}\".format(classes_vocab(prob2labels(y_valid_pred))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853cf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
